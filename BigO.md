# BigO
Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm. ([source](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/))

## Common Types
- O(1)
- O(Log N)
- O(N)
- O(N^2)
- O(2^N)
- O(N!)